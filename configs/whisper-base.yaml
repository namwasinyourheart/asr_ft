exp_manager:
  prj_name: vnpost_asr_ft
  exp_name: "whisper-base"
  exp_variant: "whisper-base_4bit"
  seed: 202502
  task_name: 'vi_asr'
  dataset_name: 
  model_name: 'openai/whisper-base'
  phase_name: 'eval'
  print_cfg: true
  exps_dir: 'exps'

data:
  is_prepared: false 
  raw_data_dir: 
  streaming: true
  columns_to_retain: ["sample_id", "audio", "text", "filename"]
  # is_dataset_dict: true
  # id_col: 'index'
  # input_col: "problem"
  # output_col: "solution"
  # context_col: 
  # do_split: true
  # subset_ratio: 1
  # val_ratio: 0.25
  # test_ratio: 0.2
  # columns_to_retain: ['index', 'answer', 'text',]
  # do_save: true
  # do_show: false
  # prepared_data_path: ./exps/llama-3.2-3b-instruct__gsm8k_vi/data/llama-3.2-3b-instruct__gsm8k_vi.pkl
model: 
  pretrained_model_name_or_path: 'openai/whisper-base'
  pretrained_processor_name_or_path: 
  load_in_4bit: true
  load_in_8bit: false
  bnb_4bit_compute_dtype: null
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_storage: "uint8"
  torch_dtype: float16
  attn_implementation: null
  device_map: null
  low_cpu_mem_usage: null
  adapter_path: null # path to lora adapter

train:
    use_peft: false
    lora:
      r: 64
      lora_alpha: 32
      lora_dropout: 0.0
      bias: none
      task_type: CAUSAL_LM
      inference_mode: false
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      modules_to_save:

    do_merge: true
    train_n_samples: 100
    val_n_samples: 10
    test_n_samples: 10
    
    train_args:
      _target_: transformers.TrainingArguments
      resume_from_checkpoint: 
      do_train: false
      do_eval: true
      do_predict: false
      learning_rate: 0.0001
      num_train_epochs: 1
      # max_steps: 1
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      # logging_strategy: "no"
      logging_steps: 1
      logging_first_step: true
      save_strategy: epoch
      eval_strategy: steps
      eval_steps: 50
      eval_accumulation_steps: 1
      eval_on_start: true
      use_cpu: false
      report_to: None

generate:
  max_new_tokens: 512
  pad_token_id: null
  skip_special_tokens: True
  temperature:
  do_sample:


evaluate:
  batch_size: 36
  break_step: 1
  do_extract_prediction: True
  prediction_file: 'test_predictions_0shot.txt'
  result_file: 'test_result_0shot.txt'

device:
  use_cpu: False
