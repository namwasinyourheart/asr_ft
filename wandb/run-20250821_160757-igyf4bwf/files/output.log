Process Rank: 0, Device: cuda:0, N_GPU: 1, Distributed Training: True
Evaluating...
You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
features:  [{'input_features': [[-0.04488348960876465, 0.6108433604240417, 0.5913799405097961, 0.33567386865615845, 0.3227959871292114, 0.44965118169784546, 0.45971399545669556, 0.5052409172058105, 0.44676709175109863, 0.5810351371765137, 0.3884674906730652, 0.23872995376586914, 0.3342973589897156, 0.33681631088256836, 0.3260958790779114, 0.17107784748077393, 0.3951295018196106, 0.35520094633102417, 0.3342741131782532, 0.2557068467140198, -0.005453228950500488, 0.3459005355834961, 0.5532536506652832, 0.2743314504623413, 0.4422597289085388, 0.4995819330215454, 0.43301236629486084, 0.3963366150856018, 0.5179944038391113, 0.509599506855011, 0.4744953513145447, 0.7164673805236816, 0.8396139144897461, 0.8549237251281738, 0.8393106460571289, 0.7382037043571472, 0.8890367746353149, 0.7773137092590332, 0.6889832019805908, 0.8146069049835205, 0.6523915529251099, 0.8434878587722778, 0.9114909172058105, 0.7096861600875854, 0.7644799947738647, 0.5120234489440918, 0.47379958629608154, 0.699690043926239, 0.779116690158844, 0.6983726024627686, 0.6327769756317139, 0.5881143808364868, 0.7028015851974487, 0.5397009253501892, 0.42669475078582764, 0.4976016879081726, 0.6791986227035522, 0.441015362739563, 0.6432667374610901, 0.670881450176239, 0.6699070930480957, 0.7655748128890991, 0.7693171501159668, 0.6945637464523315, 0.5761400461196899, 0.22482067346572876, 0.2947026491165161, 0.7166304588317871, 0.6407153606414795, 0.5822002291679382, 0.5435758829116821, 0.6095600128173828, 0.5576993227005005, 0.481697142124176, 0.6766870021820068, 0.7893608212471008, 0.5763475298881531, 0.3277215361595154, 0.5319657325744629, 0.6762189269065857, 0.7627623081207275, 0.7479587197303772, 0.8268076181411743, 0.7041484117507935, 0.6770375967025757, 0.6717689037322998, 0.5445824265480042, 0.599079966545105, 0.4956907629966736, 0.7162554860115051, 0.5504361391067505, 0.3273797035217285, 0.3441881537437439, 0.4330241084098816, 0.2824036478996277, 0.4290062189102173, 0.5002168416976929, 0.4738626480102539, 0.14853423833847046, 0.40698468685150146, 0.4844970107078552, 0.523756206035614, 0.46763843297958374, 0.21301013231277466, -0.12087953090667725, 0.4479394555091858, 0.6303278803825378, 0.5825493931770325, 0.40389710664749146, -0.04489421844482422, 0.46846550703048706, 0.5139808654785156, 0.6856269836425781, 0.6258449554443359, 0.42131268978118896, 0.582099199295044, 0.6435179710388184, 0.6551215648651123, 0.265761137008667, 0.625334620475769, 0.652642548084259, 0.674423336982727, 0.5872782468795776, 0.49473899602890015, 0.3972892761230469, 0.5597445368766785, 0.4170069694519043, 0.4274095296859741, 0.4339022636413574, 0.23938626050949097, 0.22465002536773682, 0.05620098114013672, 0.29679834842681885, 0.3066450357437134, 0.45023012161254883, 0.3689492344856262, 0.2764371633529663, 0.45965808629989624, 0.2546595335006714, 0.23763519525527954, 0.020503461360931396, 0.15432894229888916, 0.313021183013916, 0.36547160148620605, 0.3461301922798157, 0.4239484667778015, 0.48329371213912964, 0.31095975637435913, -0.003291010856628418, 0.3017680048942566, 0.4930538535118103, 0.4272405505180359, 0.22250908613204956, 0.4844801425933838, 0.3772099018096924, 0.3675159215927124, 0.005212545394897461, 0.7100095748901367, 0.42061835527420044, 0.20826518535614014, 0.2014315128326416, 0.40443187952041626, 0.5306461453437805, 0.5718701481819153, 0.38847047090530396, 0.37903010845184326, 0.3184351325035095, 0.14678853750228882, 0.08808064460754395, 0.3277633786201477, 0.5090179443359375, 0.6443899869918823, 0.5913146734237671, 0.5095818042755127, 0.6790190935134888, 0.7812426090240479, 0.740444540977478, 0.7290969491004944, 0.7038285732269287, 0.7122894525527954, 0.662304699420929, 0.3423508405685425, 0.6545750498771667, 0.8240476846694946, 0.8089669346809387, 0.8144586086273193, 0.8902134895324707, 0.7543209791183472, 0.8256094455718994, 0.4075009822845459, 0.6962658166885376, 0.48227429389953613, 0.8720046281814575, 0.9575539231300354, 0.8861823678016663, 0.8215027451515198, 0.9267433285713196, 0.8900909423828125, 0.7265620231628418, 0.3031325340270996, 0.6024804711341858, 0.6
Traceback (most recent call last):
  File "/home/nampv1/projects/vnpost_asr/finetune.py", line 386, in <module>
    main()
  File "/home/nampv1/projects/vnpost_asr/finetune.py", line 376, in main
    finetune(
  File "/home/nampv1/projects/vnpost_asr/finetune.py", line 274, in finetune
    metrics = trainer.evaluate(eval_dataset=val_ds, metric_key_prefix="eval")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/transformers/trainer.py", line 4331, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/transformers/trainer.py", line 4517, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/accelerate/data_loader.py", line 567, in __iter__
    current_batch = next(dataloader_iter)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 790, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/nampv1/projects/vnpost_asr/finetune.py", line 61, in __call__
    batch["filename"] = [f["filename"] for f in features]
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nampv1/projects/vnpost_asr/finetune.py", line 61, in <listcomp>
    batch["filename"] = [f["filename"] for f in features]
                         ~^^^^^^^^^^^^
KeyError: 'filename'
