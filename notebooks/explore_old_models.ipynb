{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04dbedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /home/nampv1/projects/vnpost_asr/voiceai.stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76706f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c0d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169e0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/nampv1/models/vnpost-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae1a907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nampv1/projects/vnpost_asr/voiceai.stt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7ca2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'voiceai.stt'\n",
      "/home/nampv1/projects/vnpost_asr/voiceai.stt\n"
     ]
    }
   ],
   "source": [
    "cd voiceai.stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00de216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nampv1/anaconda3/envs/asr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db14483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_name):\n",
    "    \"\"\"Get the model path of the model name.\"\"\"\n",
    "    middle = os.path.join(model_name, \"snapshots\")\n",
    "    items = os.listdir(middle)\n",
    "    subdirs = [d for d in items if os.path.isdir(os.path.join(middle, d))]\n",
    "\n",
    "    if not subdirs:\n",
    "        raise FileNotFoundError(f\"No models found.\")\n",
    "\n",
    "    first_subdir = subdirs[0]\n",
    "    model_path = os.path.join(middle, first_subdir)\n",
    "\n",
    "    return model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fdf5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9da501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\"\n",
    "    Load model and processor for the given model_name from its snapshot directory.\n",
    "    This function returns (model, processor).\n",
    "    \"\"\"\n",
    "    # Directory where ASR model snapshots are mounted inside Docker\n",
    "    model_folder = \"/home/nampv1/models/\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading ASR model '{model_name}' on {device}...\")\n",
    "\n",
    "    model = None\n",
    "    processor = None\n",
    "\n",
    "    try:\n",
    "        if model_name == \"vnpost-1\":\n",
    "            base_model_dir = os.path.join(model_folder, \"vnpost-2\")\n",
    "            print(f\"  Locating snapshot for base model (vnpost-2)...\")\n",
    "            base_path = get_model_path(base_model_dir)\n",
    "\n",
    "            base_processor = WhisperProcessor.from_pretrained(base_path)\n",
    "            base_model = WhisperForConditionalGeneration.from_pretrained(base_path)\n",
    "\n",
    "            adapter_dir = os.path.join(model_folder, \"vnpost-1\")\n",
    "            model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "            processor = base_processor\n",
    "\n",
    "            print(f\"Loaded '{model_name}'\")\n",
    "\n",
    "        elif model_name == \"vnpost-2\":\n",
    "            model_dir = os.path.join(model_folder, \"vnpost-2\")\n",
    "            model_path = get_model_path(model_dir)\n",
    "\n",
    "            processor = WhisperProcessor.from_pretrained(model_path)\n",
    "            model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "            print(f\"Loaded model '{model_name}'\")\n",
    "\n",
    "        elif model_name == \"vnpost-3\":\n",
    "            model_dir = os.path.join(model_folder, \"vnpost-3\")\n",
    "            model_path = get_model_path(model_dir)\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_path)\n",
    "            model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path)\n",
    "            print(f\"Loaded model '{model_name}'\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Undefined model name: {model_name}\")\n",
    "\n",
    "        # Move model to device and set to evaluation mode\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Model '{model_name}' moved to {device} and set to eval mode.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading model '{model_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045e9e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASR model 'vnpost-3' on cuda...\n",
      "Loaded model 'vnpost-3'\n",
      "Model 'vnpost-3' moved to cuda and set to eval mode.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vnpost-3\"\n",
    "model, processor = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cff568c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0c99fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec27b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path_to_process, selected_models):\n",
    "    \"\"\"\n",
    "    Transcribe the given audio file (original or enhanced) with each model in `selected_models`.\n",
    "    Returns text results (all results), list for bar chart, and individual transcriptions\n",
    "    for each model.\n",
    "    \"\"\"\n",
    "    if not selected_models:\n",
    "        return \"Vui lòng chọn ít nhất một mô hình để so sánh.\", None, None, None, []\n",
    "\n",
    "    results = []\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load audio using librosa (ensure 16kHz for Whisper)\n",
    "    try:\n",
    "        print(f\"Loading audio for transcription from: {audio_path_to_process}\")\n",
    "        # Use librosa to load and ensure 16000 Hz sample rate\n",
    "        audio_input_np, sr = librosa.load(audio_path_to_process, sr=16000)\n",
    "        if sr != 16000:\n",
    "            print(f\"Warning: Audio sample rate is {sr}Hz. Resampling to 16000Hz for Whisper.\")\n",
    "        print(f\"Audio loaded successfully, duration: {len(audio_input_np)/16000:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        return f\"Lỗi khi tải tệp âm thanh {os.path.basename(audio_path_to_process)}: {e}\", None, None, None, []\n",
    "\n",
    "    for model_name in selected_models:\n",
    "        # Clear CUDA cache and garbage collect before loading/processing EACH model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            reset_peak_memory_stats()  # Reset peak memory tracking\n",
    "        gc.collect()\n",
    "        print(f\"\\n--- Processing with Model: {model_name} ---\")\n",
    "\n",
    "        # Load model if not already loaded\n",
    "        if model_name not in loaded_models:\n",
    "            try:\n",
    "                loaded_models[model_name], loaded_processors[model_name] = load_model(model_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {model_name}: {e}\")\n",
    "                # Log the error and continue with the next model\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"transcription\": f\"[Lỗi tải mô hình: {e}]\",\n",
    "                    \"time_seconds\": 0,\n",
    "                    \"memory_mb\": 0\n",
    "                })\n",
    "                continue  # Skip this model\n",
    "\n",
    "        model = loaded_models[model_name]\n",
    "        processor = loaded_processors[model_name]\n",
    "\n",
    "        # Measure initial GPU memory\n",
    "        initial_gpu_memory = get_gpu_memory_usage()\n",
    "\n",
    "        # Sync GPU before timing starts\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Start measuring transcription time\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Prepare input\n",
    "            input_features = processor(audio_input_np, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "            gen_kwargs = {\"language\": \"vi\", \"task\": \"transcribe\"}\n",
    "\n",
    "            print(\"Generating transcript...\")\n",
    "            with torch.no_grad():\n",
    "                if hasattr(model, \"generate\"):\n",
    "                    predicted_ids = model.generate(input_features, **gen_kwargs)\n",
    "                else:\n",
    "                    print(f\"Warning: Model {model_name} may not have standard .generate method. Trying default.\")\n",
    "                    outputs = model(input_features)\n",
    "                    predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            # Decode\n",
    "            print(\"Decoding transcript...\")\n",
    "            if hasattr(processor, \"batch_decode\"):\n",
    "                transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            elif hasattr(processor, \"decode\"):\n",
    "                transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                transcription = \"[Lỗi giải mã: Bộ xử lý thiếu phương thức batch_decode hoặc decode]\"\n",
    "                print(f\"Warning: Processor for {model_name} lacks standard decoding methods.\")\n",
    "\n",
    "            # Post-processing\n",
    "            clean_transcription = spell_correct_vietnamese(\n",
    "                transcription, \n",
    "                kenlm_model, \n",
    "                symspell_telex, \n",
    "                reverse_telex_map, \n",
    "                bert_model, \n",
    "                tokenizer,\n",
    "                sym_max_telex_lookup_dist=2,  # Max distance in SymSpell lookup\n",
    "                kenlm_low_score_threshold_per_word=-4.0  # Threshold for low kenlm score\n",
    "            )\n",
    "\n",
    "            # Sync GPU before ending timing\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "\n",
    "            peak_gpu_memory = get_peak_gpu_memory_usage()\n",
    "            gpu_memory_used = peak_gpu_memory - initial_gpu_memory\n",
    "\n",
    "            print(f\"Transcript: {transcription}\")\n",
    "            print(f\"Cleaned transcript: {clean_transcription}\")\n",
    "            print(f\"Execution time: {time_taken:.2f} seconds\")\n",
    "            print(f\"GPU memory used: {gpu_memory_used:.2f} MB\")\n",
    "\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"transcription\": clean_transcription,\n",
    "                \"time_seconds\": time_taken,\n",
    "                \"memory_mb\": max(0, gpu_memory_used)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription with {model_name}: {e}\")\n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            peak_gpu_memory = get_peak_gpu_memory_usage()\n",
    "            gpu_memory_used = peak_gpu_memory - initial_gpu_memory\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"transcription\": f\"[Lỗi chuyển đổi: {e}]\",\n",
    "                \"time_seconds\": time_taken,\n",
    "                \"memory_mb\": max(0, gpu_memory_used)\n",
    "            })\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # --- Format results for UI ---\n",
    "    # Keep path in results for clarity\n",
    "    text_results = f\"Đã xử lý tệp: {os.path.basename(audio_path_to_process)}\\n\"\n",
    "    text_results += \"=\"*30 + \"\\n\\n\"\n",
    "    for r in results:\n",
    "        text_results += f\"Mô hình: {r['model']}\\n\"\n",
    "        text_results += f\"Bảng ghi: {r['transcription']}\\n\"\n",
    "        text_results += f\"Thời gian: {r['time_seconds']:.2f} giây\\n\"\n",
    "        text_results += f\"Bộ nhớ GPU: {r['memory_mb']:.2f} MB\\n\"\n",
    "        text_results += \"-\" * 30 + \"\\n\"\n",
    "    # --- End UI Result Formatting ---\n",
    "\n",
    "    model_names = [r[\"model\"] for r in results]\n",
    "    times = [r[\"time_seconds\"] for r in results]\n",
    "    memories = [r[\"memory_mb\"] for r in results]\n",
    "\n",
    "    return text_results, model_names, times, memories, [r[\"transcription\"] for r in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d549dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be08afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deploy.basic.main_nampv1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19edb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/home/nampv1/projects/vnpost_asr/data/examples/audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b810e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio for transcription from: /home/nampv1/projects/vnpost_asr/data/examples/audio.wav\n",
      "Audio loaded successfully, duration: 3.14 seconds\n",
      "\n",
      "--- Processing with Model: vnpost-3 ---\n",
      "Generating transcript...\n",
      "Decoding transcript...\n",
      "\n",
      "=== Processing Original Text: \"không thờ ơ quá và cũng không quan tâm quá trong một thời điểm.\" ===\n",
      "(Text normalized to: \"không thờ ơ quá và cũng không quan tâm quá trong một thời điểm .\")\n",
      "\n",
      "--- Detecting Errors (Telex dict lookup for single words, KenLM score check) ---\n",
      "thờ_ơ: -2.5176339149475098\n",
      "quan_tâm: -2.185436964035034\n",
      "thời_điểm: -2.1803297996520996\n",
      "No errors detected when using pyvi postagging, switching to underthesea...\n",
      "\n",
      "--- Detecting Errors (Telex dict lookup for single words, KenLM score check) ---\n",
      "--- Error Detection Complete (0 errors found) ---\n",
      "\n",
      "No potential errors detected based on KenLM scoring.\n",
      "Transcript: không thờ ơ quá và cũng không quan tâm quá trong một thời điểm.\n",
      "Cleaned transcript: không thờ ơ quá và cũng không quan tâm quá trong một thời điểm .\n",
      "Execution time: 1.02 seconds\n",
      "GPU memory used: 92.44 MB\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, r[\u001b[33m\"\u001b[39m\u001b[33mtranscription\u001b[39m\u001b[33m\"\u001b[39m], r[\u001b[33m\"\u001b[39m\u001b[33mtime_seconds\u001b[39m\u001b[33m\"\u001b[39m], r[\u001b[33m\"\u001b[39m\u001b[33mmemory_mb\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(msg)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "msg, model_names, time_vals, mem_vals, results = transcribe_audio(audio_path, [\"vnpost-3\"])\n",
    "\n",
    "if results:\n",
    "    for r in results:\n",
    "        print(r[\"model\"], r[\"transcription\"], r[\"time_seconds\"], r[\"memory_mb\"])\n",
    "else:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7b9f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý tệp: audio.wav\n",
      "==============================\n",
      "\n",
      "Mô hình: vnpost-3\n",
      "Bảng ghi: không thờ ơ quá và cũng không quan tâm quá trong một thời điểm .\n",
      "Thời gian: 1.02 giây\n",
      "Bộ nhớ GPU: 92.44 MB\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598dc651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu128'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0c12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
